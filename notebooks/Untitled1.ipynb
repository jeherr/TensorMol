{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named layers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-335b917d3168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposeAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named layers"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from layers import Dense\n",
    "import plot\n",
    "from utils import composeAll, print_\n",
    "\n",
    "\n",
    "class VAE():\n",
    "    \"\"\"Variational Autoencoder\n",
    "\n",
    "    see: Kingma & Welling - Auto-Encoding Variational Bayes\n",
    "    (http://arxiv.org/abs/1312.6114)\n",
    "    \"\"\"\n",
    "    DEFAULTS = {\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 1E-3,\n",
    "        \"dropout\": 1.,\n",
    "        \"lambda_l2_reg\": 0.,\n",
    "        \"nonlinearity\": tf.nn.elu,\n",
    "        \"squashing\": tf.nn.sigmoid\n",
    "    }\n",
    "    RESTORE_KEY = \"to_restore\"\n",
    "\n",
    "    def __init__(self, architecture=[], d_hyperparams={}, meta_graph=None,\n",
    "                 save_graph_def=True, log_dir=\"./log\"):\n",
    "        \"\"\"(Re)build a symmetric VAE model with given:\n",
    "\n",
    "            * architecture (list of nodes per encoder layer); e.g.\n",
    "               [1000, 500, 250, 10] specifies a VAE with 1000-D inputs, 10-D latents,\n",
    "               & end-to-end architecture [1000, 500, 250, 10, 250, 500, 1000]\n",
    "\n",
    "            * hyperparameters (optional dictionary of updates to `DEFAULTS`)\n",
    "        \"\"\"\n",
    "        self.architecture = architecture\n",
    "        self.__dict__.update(VAE.DEFAULTS, **d_hyperparams)\n",
    "        self.sesh = tf.Session()\n",
    "\n",
    "        if not meta_graph: # new model\n",
    "            self.datetime = datetime.now().strftime(r\"%y%m%d_%H%M\")\n",
    "            assert len(self.architecture) > 2, \\\n",
    "                \"Architecture must have more layers! (input, 1+ hidden, latent)\"\n",
    "\n",
    "            # build graph\n",
    "            handles = self._buildGraph()\n",
    "            for handle in handles:\n",
    "                tf.add_to_collection(VAE.RESTORE_KEY, handle)\n",
    "            self.sesh.run(tf.initialize_all_variables())\n",
    "\n",
    "        else: # restore saved model\n",
    "            model_datetime, model_name = os.path.basename(meta_graph).split(\"_vae_\")\n",
    "            self.datetime = \"{}_reloaded\".format(model_datetime)\n",
    "            model_architecture, _ = re.split(\"_|-\", model_name)\n",
    "            self.architecture = [int(n) for n in model_architecture]\n",
    "\n",
    "            # rebuild graph\n",
    "            meta_graph = os.path.abspath(meta_graph)\n",
    "            tf.train.import_meta_graph(meta_graph + \".meta\").restore(\n",
    "                self.sesh, meta_graph)\n",
    "            handles = self.sesh.graph.get_collection(VAE.RESTORE_KEY)\n",
    "\n",
    "        # unpack handles for tensor ops to feed or fetch\n",
    "        (self.x_in, self.dropout_, self.z_mean, self.z_log_sigma,\n",
    "         self.x_reconstructed, self.z_, self.x_reconstructed_,\n",
    "         self.cost, self.global_step, self.train_op) = handles\n",
    "\n",
    "        if save_graph_def: # tensorboard\n",
    "            self.logger = tf.train.SummaryWriter(log_dir, self.sesh.graph)\n",
    "\n",
    "    @property\n",
    "    def step(self):\n",
    "        \"\"\"Train step\"\"\"\n",
    "        return self.global_step.eval(session=self.sesh)\n",
    "\n",
    "    def _buildGraph(self):\n",
    "        x_in = tf.placeholder(tf.float32, shape=[None, # enables variable batch size\n",
    "                                                 self.architecture[0]], name=\"x\")\n",
    "        dropout = tf.placeholder_with_default(1., shape=[], name=\"dropout\")\n",
    "\n",
    "        # encoding / \"recognition\": q(z|x)\n",
    "        encoding = [Dense(\"encoding\", hidden_size, dropout, self.nonlinearity)\n",
    "                    # hidden layers reversed for function composition: outer -> inner\n",
    "                    for hidden_size in reversed(self.architecture[1:-1])]\n",
    "        h_encoded = composeAll(encoding)(x_in)\n",
    "\n",
    "        # latent distribution parameterized by hidden encoding\n",
    "        # z ~ N(z_mean, np.exp(z_log_sigma)**2)\n",
    "        z_mean = Dense(\"z_mean\", self.architecture[-1], dropout)(h_encoded)\n",
    "        z_log_sigma = Dense(\"z_log_sigma\", self.architecture[-1], dropout)(h_encoded)\n",
    "\n",
    "        # kingma & welling: only 1 draw necessary as long as minibatch large enough (>100)\n",
    "        z = self.sampleGaussian(z_mean, z_log_sigma)\n",
    "\n",
    "        # decoding / \"generative\": p(x|z)\n",
    "        decoding = [Dense(\"decoding\", hidden_size, dropout, self.nonlinearity)\n",
    "                    for hidden_size in self.architecture[1:-1]] # assumes symmetry\n",
    "        # final reconstruction: restore original dims, squash outputs [0, 1]\n",
    "        decoding.insert(0, Dense( # prepend as outermost function\n",
    "            \"x_decoding\", self.architecture[0], dropout, self.squashing))\n",
    "        x_reconstructed = tf.identity(composeAll(decoding)(z), name=\"x_reconstructed\")\n",
    "\n",
    "        # reconstruction loss: mismatch b/w x & x_reconstructed\n",
    "        # binary cross-entropy -- assumes x & p(x|z) are iid Bernoullis\n",
    "        rec_loss = VAE.crossEntropy(x_reconstructed, x_in)\n",
    "\n",
    "        # Kullback-Leibler divergence: mismatch b/w approximate vs. imposed/true posterior\n",
    "        kl_loss = VAE.kullbackLeibler(z_mean, z_log_sigma)\n",
    "\n",
    "        with tf.name_scope(\"l2_regularization\"):\n",
    "            regularizers = [tf.nn.l2_loss(var) for var in self.sesh.graph.get_collection(\n",
    "                \"trainable_variables\") if \"weights\" in var.name]\n",
    "            l2_reg = self.lambda_l2_reg * tf.add_n(regularizers)\n",
    "\n",
    "        with tf.name_scope(\"cost\"):\n",
    "            # average over minibatch\n",
    "            cost = tf.reduce_mean(rec_loss + kl_loss, name=\"vae_cost\")\n",
    "            cost += l2_reg\n",
    "\n",
    "        # optimization\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        with tf.name_scope(\"Adam_optimizer\"):\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads_and_vars = optimizer.compute_gradients(cost, tvars)\n",
    "            clipped = [(tf.clip_by_value(grad, -5, 5), tvar) # gradient clipping\n",
    "                    for grad, tvar in grads_and_vars]\n",
    "            train_op = optimizer.apply_gradients(clipped, global_step=global_step,\n",
    "                                                 name=\"minimize_cost\")\n",
    "\n",
    "        # ops to directly explore latent space\n",
    "        # defaults to prior z ~ N(0, I)\n",
    "        with tf.name_scope(\"latent_in\"):\n",
    "            z_ = tf.placeholder_with_default(tf.random_normal([1, self.architecture[-1]]),\n",
    "                                            shape=[None, self.architecture[-1]],\n",
    "                                            name=\"latent_in\")\n",
    "        x_reconstructed_ = composeAll(decoding)(z_)\n",
    "\n",
    "        return (x_in, dropout, z_mean, z_log_sigma, x_reconstructed,\n",
    "                z_, x_reconstructed_, cost, global_step, train_op)\n",
    "\n",
    "    def sampleGaussian(self, mu, log_sigma):\n",
    "        \"\"\"(Differentiably!) draw sample from Gaussian with given shape, subject to random noise epsilon\"\"\"\n",
    "        with tf.name_scope(\"sample_gaussian\"):\n",
    "            # reparameterization trick\n",
    "            epsilon = tf.random_normal(tf.shape(log_sigma), name=\"epsilon\")\n",
    "            return mu + epsilon * tf.exp(log_sigma) # N(mu, I * sigma**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def crossEntropy(obs, actual, offset=1e-7):\n",
    "        \"\"\"Binary cross-entropy, per training example\"\"\"\n",
    "        # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            # bound by clipping to avoid nan\n",
    "            obs_ = tf.clip_by_value(obs, offset, 1 - offset)\n",
    "            return -tf.reduce_sum(actual * tf.log(obs_) +\n",
    "                                  (1 - actual) * tf.log(1 - obs_), 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def l1_loss(obs, actual):\n",
    "        \"\"\"L1 loss (a.k.a. LAD), per training example\"\"\"\n",
    "        # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "        with tf.name_scope(\"l1_loss\"):\n",
    "            return tf.reduce_sum(tf.abs(obs - actual) , 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_loss(obs, actual):\n",
    "        \"\"\"L2 loss (a.k.a. Euclidean / LSE), per training example\"\"\"\n",
    "        # (tf.Tensor, tf.Tensor, float) -> tf.Tensor\n",
    "        with tf.name_scope(\"l2_loss\"):\n",
    "            return tf.reduce_sum(tf.square(obs - actual), 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def kullbackLeibler(mu, log_sigma):\n",
    "        \"\"\"(Gaussian) Kullback-Leibler divergence KL(q||p), per training example\"\"\"\n",
    "        # (tf.Tensor, tf.Tensor) -> tf.Tensor\n",
    "        with tf.name_scope(\"KL_divergence\"):\n",
    "            # = -0.5 * (1 + log(sigma**2) - mu**2 - sigma**2)\n",
    "            return -0.5 * tf.reduce_sum(1 + 2 * log_sigma - mu**2 -\n",
    "                                        tf.exp(2 * log_sigma), 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Probabilistic encoder from inputs to latent distribution parameters;\n",
    "        a.k.a. inference network q(z|x)\n",
    "        \"\"\"\n",
    "        # np.array -> [float, float]\n",
    "        feed_dict = {self.x_in: x}\n",
    "        return self.sesh.run([self.z_mean, self.z_log_sigma], feed_dict=feed_dict)\n",
    "\n",
    "    def decode(self, zs=None):\n",
    "        \"\"\"Generative decoder from latent space to reconstructions of input space;\n",
    "        a.k.a. generative network p(x|z)\n",
    "        \"\"\"\n",
    "        # (np.array | tf.Variable) -> np.array\n",
    "        feed_dict = dict()\n",
    "        if zs is not None:\n",
    "            is_tensor = lambda x: hasattr(x, \"eval\")\n",
    "            zs = (self.sesh.run(zs) if is_tensor(zs) else zs) # coerce to np.array\n",
    "            feed_dict.update({self.z_: zs})\n",
    "        # else, zs defaults to draw from conjugate prior z ~ N(0, I)\n",
    "        return self.sesh.run(self.x_reconstructed_, feed_dict=feed_dict)\n",
    "\n",
    "    def vae(self, x):\n",
    "        \"\"\"End-to-end autoencoder\"\"\"\n",
    "        # np.array -> np.array\n",
    "        return self.decode(self.sampleGaussian(*self.encode(x)))\n",
    "\n",
    "    def train(self, X, max_iter=np.inf, max_epochs=np.inf, cross_validate=True,\n",
    "              verbose=True, save=True, outdir=\"./out\", plots_outdir=\"./png\",\n",
    "              plot_latent_over_time=False):\n",
    "        if save:\n",
    "            saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        try:\n",
    "            err_train = 0\n",
    "            now = datetime.now().isoformat()[11:]\n",
    "            print(\"------- Training begin: {} -------\\n\".format(now))\n",
    "\n",
    "            if plot_latent_over_time: # plot latent space over log_BASE time\n",
    "                BASE = 2\n",
    "                INCREMENT = 0.5\n",
    "                pow_ = 0\n",
    "\n",
    "            while True:\n",
    "                x, _ = X.train.next_batch(self.batch_size)\n",
    "                feed_dict = {self.x_in: x, self.dropout_: self.dropout}\n",
    "                fetches = [self.x_reconstructed, self.cost, self.global_step, self.train_op]\n",
    "                x_reconstructed, cost, i, _ = self.sesh.run(fetches, feed_dict)\n",
    "\n",
    "                err_train += cost\n",
    "\n",
    "                if plot_latent_over_time:\n",
    "                    while int(round(BASE**pow_)) == i:\n",
    "                        plot.exploreLatent(self, nx=30, ny=30, ppf=True, outdir=plots_outdir,\n",
    "                                           name=\"explore_ppf30_{}\".format(pow_))\n",
    "\n",
    "                        names = (\"train\", \"validation\", \"test\")\n",
    "                        datasets = (X.train, X.validation, X.test)\n",
    "                        for name, dataset in zip(names, datasets):\n",
    "                            plot.plotInLatent(self, dataset.images, dataset.labels, range_=\n",
    "                                              (-6, 6), title=name, outdir=plots_outdir,\n",
    "                                              name=\"{}_{}\".format(name, pow_))\n",
    "\n",
    "                        print(\"{}^{} = {}\".format(BASE, pow_, i))\n",
    "                        pow_ += INCREMENT\n",
    "\n",
    "                if i%1000 == 0 and verbose:\n",
    "                    print(\"round {} --> avg cost: \".format(i), err_train / i)\n",
    "\n",
    "                if i%2000 == 0 and verbose:# and i >= 10000:\n",
    "                    # visualize `n` examples of current minibatch inputs + reconstructions\n",
    "                    plot.plotSubset(self, x, x_reconstructed, n=10, name=\"train\",\n",
    "                                    outdir=plots_outdir)\n",
    "\n",
    "                    if cross_validate:\n",
    "                        x, _ = X.validation.next_batch(self.batch_size)\n",
    "                        feed_dict = {self.x_in: x}\n",
    "                        fetches = [self.x_reconstructed, self.cost]\n",
    "                        x_reconstructed, cost = self.sesh.run(fetches, feed_dict)\n",
    "\n",
    "                        print(\"round {} --> CV cost: \".format(i), cost)\n",
    "                        plot.plotSubset(self, x, x_reconstructed, n=10, name=\"cv\",\n",
    "                                        outdir=plots_outdir)\n",
    "\n",
    "                if i >= max_iter or X.train.epochs_completed >= max_epochs:\n",
    "                    print(\"final avg cost (@ step {} = epoch {}): {}\".format(\n",
    "                        i, X.train.epochs_completed, err_train / i))\n",
    "                    now = datetime.now().isoformat()[11:]\n",
    "                    print(\"------- Training end: {} -------\\n\".format(now))\n",
    "\n",
    "                    if save:\n",
    "                        outfile = os.path.join(os.path.abspath(outdir), \"{}_vae_{}\".format(\n",
    "                            self.datetime, \"_\".join(map(str, self.architecture))))\n",
    "                        saver.save(self.sesh, outfile, global_step=self.step)\n",
    "                    try:\n",
    "                        self.logger.flush()\n",
    "                        self.logger.close()\n",
    "                    except(AttributeError): # not logging\n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "        except(KeyboardInterrupt):\n",
    "            print(\"final avg cost (@ step {} = epoch {}): {}\".format(\n",
    "                i, X.train.epochs_completed, err_train / i))\n",
    "            now = datetime.now().isoformat()[11:]\n",
    "            print(\"------- Training end: {} -------\\n\".format(now))\n",
    "            sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
